{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 5), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 5)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'load'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-5preds10-load-100weight'\n",
    "REPLAY_NAME = 'replay-5preds10-load-100weight'\n",
    "ELAPSE_NAME = 'elapse-5preds10-load-100weight'\n",
    "REWARDS_NAME = 'rewards-5preds10-load-100weight'\n",
    "COLLISIONS_NAME = 'collsions-5preds10-load-100weight'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = '5preds-10w/model-5pred-2cues-500trials-10weight-2603626.ckpt'\n",
    "REPLAY_RE = '5preds-10w/replay-5pred-2cues-500trials-10weight-2603626.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "collisions = []\n",
    "timesteps = [0]\n",
    "acts = []\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE \n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE \n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards,\n",
    "                 percent = 1,\n",
    "                 all_actions = acts)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "        \n",
    "    # Save collisions\n",
    "    collisions_name = 'saved_graphs/' + COLLISIONS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(collisions_name, \"wb\") as f:\n",
    "        pickle.dump(g.collisions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reset\n",
    "%reset -f\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# doing stuff\n",
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 5), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 5)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'load'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-5preds20-load-100weight'\n",
    "REPLAY_NAME = 'replay-5preds20-load-100weight'\n",
    "ELAPSE_NAME = 'elapse-5preds20-load-100weight'\n",
    "REWARDS_NAME = 'rewards-5preds20-load-100weight'\n",
    "COLLISIONS_NAME = 'collsions-5preds20-load-100weight'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = '5preds-20w/model-5pred-2cues-500trials-20weight-1561231.ckpt'\n",
    "REPLAY_RE = '5preds-20w/replay-5pred-2cues-500trials-20weight-1561231.pkl'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "        \n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE \n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE \n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards,\n",
    "                 percent = 1)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "        \n",
    "    # Save collisions\n",
    "    collisions_name = 'saved_graphs/' + COLLISIONS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(collisions_name, \"wb\") as f:\n",
    "        pickle.dump(g.collisions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reset\n",
    "%reset -f\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# doing stuff\n",
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 5), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 5)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'load'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-5preds30-load-100weight'\n",
    "REPLAY_NAME = 'replay-5preds30-load-100weight'\n",
    "ELAPSE_NAME = 'elapse-5preds30-load-100weight'\n",
    "REWARDS_NAME = 'rewards-5preds30-load-100weight'\n",
    "COLLISIONS_NAME = 'collsions-5preds30-load-100weight'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = '5preds-30w/model-5pred-2cues-500trials-30weight-903616.ckpt'\n",
    "REPLAY_RE = '5preds-30w/replay-5pred-2cues-500trials-30weight-903616.pkl'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "        \n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE \n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE \n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards,\n",
    "                 percent = 1)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "        \n",
    "    # Save collisions\n",
    "    collisions_name = 'saved_graphs/' + COLLISIONS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(collisions_name, \"wb\") as f:\n",
    "        pickle.dump(g.collisions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset\n",
    "%reset -f\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# doing stuff\n",
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 5), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 5)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'load'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-5preds40-load-100weight'\n",
    "REPLAY_NAME = 'replay-5preds40-load-100weight'\n",
    "ELAPSE_NAME = 'elapse-5preds40-load-100weight'\n",
    "REWARDS_NAME = 'rewards-5preds40-load-100weight'\n",
    "COLLISIONS_NAME = 'collsions-5preds40-load-100weight'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = '5preds-40w/model-5pred-2cues-500trials-40weight-579358.ckpt'\n",
    "REPLAY_RE = '5preds-40w/replay-5pred-2cues-500trials-40weight-579358.pkl'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "        \n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE \n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE \n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards,\n",
    "                 percent = 1)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "        \n",
    "    # Save collisions\n",
    "    collisions_name = 'saved_graphs/' + COLLISIONS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(collisions_name, \"wb\") as f:\n",
    "        pickle.dump(g.collisions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset\n",
    "%reset -f\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# doing stuff\n",
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 5), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 5)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'load'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-5preds50-load-100weight'\n",
    "REPLAY_NAME = 'replay-5preds50-load-100weight'\n",
    "ELAPSE_NAME = 'elapse-5preds50-load-100weight'\n",
    "REWARDS_NAME = 'rewards-5preds50-load-100weight'\n",
    "COLLISIONS_NAME = 'collsions-5preds50-load-100weight'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = '5preds-50w/model-5pred-2cues-500trials-50weight-516364.ckpt'\n",
    "REPLAY_RE = '5preds-50w/replay-5pred-2cues-500trials-50weight-516364.pkl'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "        \n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE \n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE \n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards,\n",
    "                 percent = 1)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "        \n",
    "    # Save collisions\n",
    "    collisions_name = 'saved_graphs/' + COLLISIONS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(collisions_name, \"wb\") as f:\n",
    "        pickle.dump(g.collisions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset\n",
    "%reset -f\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# doing stuff\n",
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 5), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 5)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'load'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-5preds60-load-100weight'\n",
    "REPLAY_NAME = 'replay-5preds60-load-100weight'\n",
    "ELAPSE_NAME = 'elapse-5preds60-load-100weight'\n",
    "REWARDS_NAME = 'rewards-5preds60-load-100weight'\n",
    "COLLISIONS_NAME = 'collsions-5preds60-load-100weight'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = '5preds-60w/model-5pred-2cues-500trials-60weight-343942.ckpt'\n",
    "REPLAY_RE = '5preds-60w/replay-5pred-2cues-500trials-60weight-343942.pkl'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "        \n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE \n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE \n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards,\n",
    "                 percent = 1)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "        \n",
    "    # Save collisions\n",
    "    collisions_name = 'saved_graphs/' + COLLISIONS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(collisions_name, \"wb\") as f:\n",
    "        pickle.dump(g.collisions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset\n",
    "%reset -f\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# doing stuff\n",
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 5), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 5)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'load'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-5preds80-load-100weight'\n",
    "REPLAY_NAME = 'replay-5preds80-load-100weight'\n",
    "ELAPSE_NAME = 'elapse-5preds80-load-100weight'\n",
    "REWARDS_NAME = 'rewards-5preds80-load-100weight'\n",
    "COLLISIONS_NAME = 'collsions-5preds80-load-100weight'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = '5preds-80w/model-5pred-2cues-500trials-80weight-238297.ckpt'\n",
    "REPLAY_RE = '5preds-80w/replay-5pred-2cues-500trials-80weight-238297.pkl'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "        \n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE \n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE \n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards,\n",
    "                 percent = 1)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "        \n",
    "    # Save collisions\n",
    "    collisions_name = 'saved_graphs/' + COLLISIONS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(collisions_name, \"wb\") as f:\n",
    "        pickle.dump(g.collisions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset\n",
    "%reset -f\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# doing stuff\n",
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 5), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 5)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'load'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-5preds90-load-100weight'\n",
    "REPLAY_NAME = 'replay-5preds90-load-100weight'\n",
    "ELAPSE_NAME = 'elapse-5preds90-load-100weight'\n",
    "REWARDS_NAME = 'rewards-5preds90-load-100weight'\n",
    "COLLISIONS_NAME = 'collsions-5preds90-load-100weight'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = '5preds-90w/model-5pred-2cues-500trials-90weight-180703.ckpt'\n",
    "REPLAY_RE = '5preds-90w/replay-5pred-2cues-500trials-90weight-180703.pkl'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "        \n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE \n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE \n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards,\n",
    "                 percent = 1)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "        \n",
    "    # Save collisions\n",
    "    collisions_name = 'saved_graphs/' + COLLISIONS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(collisions_name, \"wb\") as f:\n",
    "        pickle.dump(g.collisions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset\n",
    "%reset -f\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# doing stuff\n",
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 5), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 5)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'load'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-5preds100-load-100weight'\n",
    "REPLAY_NAME = 'replay-5preds100-load-100weight'\n",
    "ELAPSE_NAME = 'elapse-5preds100-load-100weight'\n",
    "REWARDS_NAME = 'rewards-5preds100-load-100weight'\n",
    "COLLISIONS_NAME = 'collsions-5preds100-load-100weight'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = '5preds-free/model-5pred-2cues-500trials-189205.ckpt'\n",
    "REPLAY_RE = '5preds-free/replay-5pred-2cues-500trials-189205.pkl'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "        \n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE \n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE \n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards,\n",
    "                 percent = 1)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "        \n",
    "    # Save collisions\n",
    "    collisions_name = 'saved_graphs/' + COLLISIONS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(collisions_name, \"wb\") as f:\n",
    "        pickle.dump(g.collisions, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (dqn-multiagent)",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
