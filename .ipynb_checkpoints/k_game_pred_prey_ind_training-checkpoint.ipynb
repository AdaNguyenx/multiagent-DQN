{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 14:23:27,150] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 14:23:27,297] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "4\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "10\n",
      "13\n",
      "17\n",
      "22\n",
      "25\n",
      "25\n",
      "27\n",
      "30\n",
      "32\n",
      "37\n",
      "37\n",
      "39\n",
      "42\n",
      "45\n",
      "45\n",
      "49\n",
      "57\n",
      "60\n",
      "63\n",
      "64\n",
      "66\n",
      "70\n",
      "75\n",
      "83\n",
      "93\n",
      "97\n",
      "104\n",
      "115\n",
      "121\n",
      "128\n",
      "137\n",
      "152\n",
      "166\n",
      "178\n",
      "192\n",
      "206\n",
      "218\n",
      "234\n",
      "247\n",
      "280\n",
      "303\n",
      "329\n",
      "350\n",
      "377\n",
      "402\n",
      "427\n",
      "460\n",
      "493\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-10'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-10'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-10'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-10'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-191685'\n",
    "REPLAY_RE = 'replay-1-191685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred10'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred10'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 14:35:41,122] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 14:35:41,204] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "9\n",
      "12\n",
      "13\n",
      "16\n",
      "16\n",
      "19\n",
      "23\n",
      "26\n",
      "29\n",
      "38\n",
      "44\n",
      "52\n",
      "68\n",
      "80\n",
      "93\n",
      "103\n",
      "114\n",
      "132\n",
      "146\n",
      "157\n",
      "168\n",
      "185\n",
      "214\n",
      "244\n",
      "261\n",
      "278\n",
      "288\n",
      "313\n",
      "348\n",
      "365\n",
      "382\n",
      "409\n",
      "444\n",
      "468\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-11'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-11'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-11'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-11'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-191685'\n",
    "REPLAY_RE = 'replay-1-191685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred11'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred11'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 14:45:15,914] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 14:45:15,980] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "7\n",
      "9\n",
      "10\n",
      "10\n",
      "12\n",
      "14\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "23\n",
      "23\n",
      "24\n",
      "26\n",
      "27\n",
      "29\n",
      "33\n",
      "33\n",
      "33\n",
      "39\n",
      "40\n",
      "42\n",
      "47\n",
      "49\n",
      "52\n",
      "58\n",
      "63\n",
      "70\n",
      "73\n",
      "73\n",
      "76\n",
      "81\n",
      "83\n",
      "87\n",
      "95\n",
      "101\n",
      "105\n",
      "110\n",
      "114\n",
      "119\n",
      "121\n",
      "125\n",
      "127\n",
      "131\n",
      "134\n",
      "138\n",
      "154\n",
      "162\n",
      "175\n",
      "186\n",
      "200\n",
      "221\n",
      "244\n",
      "266\n",
      "268\n",
      "288\n",
      "308\n",
      "324\n",
      "344\n",
      "376\n",
      "417\n",
      "457\n",
      "492\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-12'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-12'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-12'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-12'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-191685'\n",
    "REPLAY_RE = 'replay-1-191685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred12'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred12'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 15:01:42,701] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 15:01:42,778] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "11\n",
      "14\n",
      "14\n",
      "15\n",
      "18\n",
      "19\n",
      "19\n",
      "20\n",
      "24\n",
      "27\n",
      "33\n",
      "38\n",
      "43\n",
      "52\n",
      "64\n",
      "71\n",
      "83\n",
      "92\n",
      "102\n",
      "121\n",
      "130\n",
      "140\n",
      "151\n",
      "175\n",
      "196\n",
      "215\n",
      "236\n",
      "245\n",
      "276\n",
      "314\n",
      "348\n",
      "391\n",
      "418\n",
      "458\n",
      "493\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-13'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-13'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-13'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-13'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-191685'\n",
    "REPLAY_RE = 'replay-1-191685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred13'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred13'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 15:09:50,243] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 15:09:50,310] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "18\n",
      "21\n",
      "22\n",
      "26\n",
      "26\n",
      "34\n",
      "39\n",
      "44\n",
      "46\n",
      "51\n",
      "61\n",
      "81\n",
      "84\n",
      "97\n",
      "110\n",
      "116\n",
      "124\n",
      "132\n",
      "155\n",
      "170\n",
      "187\n",
      "199\n",
      "220\n",
      "238\n",
      "250\n",
      "258\n",
      "283\n",
      "307\n",
      "340\n",
      "365\n",
      "391\n",
      "435\n",
      "473\n",
      "486\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-14'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-14'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-14'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-14'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-191685'\n",
    "REPLAY_RE = 'replay-1-191685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred14'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred14'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 15:18:56,851] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 15:18:56,917] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "5\n",
      "7\n",
      "7\n",
      "9\n",
      "11\n",
      "13\n",
      "13\n",
      "13\n",
      "17\n",
      "18\n",
      "19\n",
      "21\n",
      "23\n",
      "26\n",
      "28\n",
      "28\n",
      "30\n",
      "33\n",
      "38\n",
      "42\n",
      "42\n",
      "44\n",
      "49\n",
      "51\n",
      "51\n",
      "51\n",
      "56\n",
      "56\n",
      "57\n",
      "59\n",
      "60\n",
      "62\n",
      "63\n",
      "66\n",
      "68\n",
      "70\n",
      "76\n",
      "80\n",
      "84\n",
      "86\n",
      "93\n",
      "99\n",
      "104\n",
      "113\n",
      "121\n",
      "127\n",
      "136\n",
      "140\n",
      "146\n",
      "154\n",
      "170\n",
      "173\n",
      "173\n",
      "181\n",
      "186\n",
      "189\n",
      "189\n",
      "189\n",
      "189\n",
      "189\n",
      "189\n",
      "189\n",
      "192\n",
      "192\n",
      "194\n",
      "211\n",
      "239\n",
      "268\n",
      "301\n",
      "317\n",
      "322\n",
      "335\n",
      "335\n",
      "335\n",
      "337\n",
      "344\n",
      "344\n",
      "344\n",
      "344\n",
      "344\n",
      "344\n",
      "344\n",
      "344\n",
      "344\n",
      "344\n",
      "344\n",
      "344\n",
      "344\n",
      "346\n",
      "346\n",
      "346\n",
      "346\n",
      "348\n",
      "359\n",
      "364\n",
      "381\n",
      "394\n",
      "412\n",
      "425\n",
      "434\n",
      "435\n",
      "455\n",
      "474\n",
      "480\n",
      "486\n",
      "498\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-15'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-15'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-15'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-15'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-191685'\n",
    "REPLAY_RE = 'replay-1-191685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred15'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred15'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 15:42:14,167] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 15:42:14,249] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "12\n",
      "14\n",
      "14\n",
      "15\n",
      "16\n",
      "19\n",
      "19\n",
      "21\n",
      "22\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "24\n",
      "24\n",
      "24\n",
      "25\n",
      "27\n",
      "27\n",
      "28\n",
      "29\n",
      "29\n",
      "32\n",
      "33\n",
      "35\n",
      "35\n",
      "36\n",
      "37\n",
      "37\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "41\n",
      "41\n",
      "44\n",
      "48\n",
      "54\n",
      "64\n",
      "71\n",
      "77\n",
      "78\n",
      "85\n",
      "91\n",
      "96\n",
      "112\n",
      "138\n",
      "150\n",
      "177\n",
      "194\n",
      "217\n",
      "246\n",
      "269\n",
      "269\n",
      "304\n",
      "318\n",
      "320\n",
      "329\n",
      "329\n",
      "343\n",
      "378\n",
      "388\n",
      "401\n",
      "417\n",
      "433\n",
      "476\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-16'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-16'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-16'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-16'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-191685'\n",
    "REPLAY_RE = 'replay-1-191685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred16'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred16'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 15:57:59,126] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 15:57:59,196] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "15\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "19\n",
      "24\n",
      "27\n",
      "30\n",
      "35\n",
      "39\n",
      "49\n",
      "56\n",
      "60\n",
      "68\n",
      "72\n",
      "78\n",
      "87\n",
      "97\n",
      "112\n",
      "116\n",
      "126\n",
      "131\n",
      "144\n",
      "165\n",
      "178\n",
      "190\n",
      "201\n",
      "218\n",
      "240\n",
      "266\n",
      "277\n",
      "289\n",
      "316\n",
      "339\n",
      "359\n",
      "387\n",
      "427\n",
      "442\n",
      "464\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-17'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-17'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-17'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-17'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-191685'\n",
    "REPLAY_RE = 'replay-1-191685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred17'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred17'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 16:09:45,427] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 16:09:45,506] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "11\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "14\n",
      "16\n",
      "21\n",
      "24\n",
      "25\n",
      "29\n",
      "34\n",
      "36\n",
      "40\n",
      "42\n",
      "45\n",
      "50\n",
      "53\n",
      "57\n",
      "58\n",
      "58\n",
      "61\n",
      "67\n",
      "69\n",
      "71\n",
      "75\n",
      "80\n",
      "84\n",
      "88\n",
      "97\n",
      "101\n",
      "107\n",
      "110\n",
      "116\n",
      "121\n",
      "128\n",
      "139\n",
      "146\n",
      "157\n",
      "172\n",
      "192\n",
      "205\n",
      "217\n",
      "229\n",
      "247\n",
      "258\n",
      "276\n",
      "293\n",
      "315\n",
      "329\n",
      "346\n",
      "363\n",
      "387\n",
      "414\n",
      "445\n",
      "478\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-18'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-18'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-18'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-18'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-191685'\n",
    "REPLAY_RE = 'replay-1-191685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred18'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred18'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 16:23:52,999] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 16:23:53,074] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "9\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "14\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "17\n",
      "17\n",
      "21\n",
      "22\n",
      "24\n",
      "25\n",
      "28\n",
      "29\n",
      "29\n",
      "29\n",
      "29\n",
      "30\n",
      "32\n",
      "34\n",
      "35\n",
      "35\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "39\n",
      "41\n",
      "41\n",
      "41\n",
      "44\n",
      "44\n",
      "51\n",
      "57\n",
      "65\n",
      "70\n",
      "75\n",
      "77\n",
      "78\n",
      "80\n",
      "85\n",
      "86\n",
      "86\n",
      "87\n",
      "90\n",
      "94\n",
      "95\n",
      "95\n",
      "99\n",
      "105\n",
      "108\n",
      "113\n",
      "114\n",
      "119\n",
      "132\n",
      "143\n",
      "158\n",
      "178\n",
      "190\n",
      "198\n",
      "215\n",
      "232\n",
      "246\n",
      "255\n",
      "259\n",
      "272\n",
      "289\n",
      "302\n",
      "325\n",
      "346\n",
      "363\n",
      "399\n",
      "428\n",
      "451\n",
      "478\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-19'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-19'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-19'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-19'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-191685'\n",
    "REPLAY_RE = 'replay-1-191685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred19'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred19'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 16:41:18,743] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 16:41:18,810] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "4\n",
      "6\n",
      "8\n",
      "8\n",
      "10\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "20\n",
      "20\n",
      "20\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "24\n",
      "28\n",
      "28\n",
      "32\n",
      "33\n",
      "38\n",
      "40\n",
      "44\n",
      "48\n",
      "50\n",
      "52\n",
      "56\n",
      "59\n",
      "60\n",
      "61\n",
      "65\n",
      "74\n",
      "76\n",
      "80\n",
      "83\n",
      "91\n",
      "92\n",
      "95\n",
      "106\n",
      "108\n",
      "112\n",
      "117\n",
      "121\n",
      "129\n",
      "140\n",
      "144\n",
      "149\n",
      "158\n",
      "165\n",
      "172\n",
      "181\n",
      "191\n",
      "198\n",
      "213\n",
      "220\n",
      "225\n",
      "239\n",
      "252\n",
      "256\n",
      "266\n",
      "287\n",
      "296\n",
      "314\n",
      "314\n",
      "318\n",
      "318\n",
      "330\n",
      "335\n",
      "343\n",
      "369\n",
      "383\n",
      "407\n",
      "427\n",
      "448\n",
      "473\n",
      "486\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-20'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-20'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-20'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-20'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-201685'\n",
    "REPLAY_RE = 'replay-1-201685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred20'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred20'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 16:59:11,541] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 16:59:11,615] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "4\n",
      "9\n",
      "12\n",
      "13\n",
      "16\n",
      "18\n",
      "20\n",
      "20\n",
      "31\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "40\n",
      "46\n",
      "53\n",
      "60\n",
      "61\n",
      "69\n",
      "82\n",
      "87\n",
      "92\n",
      "101\n",
      "103\n",
      "103\n",
      "111\n",
      "113\n",
      "115\n",
      "119\n",
      "133\n",
      "140\n",
      "144\n",
      "144\n",
      "149\n",
      "152\n",
      "158\n",
      "175\n",
      "194\n",
      "205\n",
      "215\n",
      "223\n",
      "232\n",
      "235\n",
      "251\n",
      "262\n",
      "276\n",
      "288\n",
      "298\n",
      "304\n",
      "315\n",
      "342\n",
      "365\n",
      "402\n",
      "448\n",
      "486\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-21'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-21'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-21'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-21'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-211685'\n",
    "REPLAY_RE = 'replay-1-211685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred21'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred21'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 17:10:29,014] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 17:10:29,081] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "7\n",
      "10\n",
      "16\n",
      "18\n",
      "18\n",
      "21\n",
      "24\n",
      "28\n",
      "30\n",
      "34\n",
      "39\n",
      "43\n",
      "43\n",
      "46\n",
      "50\n",
      "57\n",
      "59\n",
      "60\n",
      "64\n",
      "66\n",
      "70\n",
      "72\n",
      "75\n",
      "79\n",
      "81\n",
      "83\n",
      "88\n",
      "98\n",
      "99\n",
      "104\n",
      "110\n",
      "113\n",
      "118\n",
      "124\n",
      "134\n",
      "142\n",
      "144\n",
      "152\n",
      "153\n",
      "163\n",
      "168\n",
      "176\n",
      "186\n",
      "196\n",
      "211\n",
      "219\n",
      "230\n",
      "244\n",
      "255\n",
      "267\n",
      "275\n",
      "286\n",
      "313\n",
      "349\n",
      "371\n",
      "408\n",
      "443\n",
      "492\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-22'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-22'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-22'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-22'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-221685'\n",
    "REPLAY_RE = 'replay-1-221685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred22'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred22'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 17:23:21,534] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 17:23:21,605] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "6\n",
      "9\n",
      "9\n",
      "10\n",
      "12\n",
      "14\n",
      "15\n",
      "16\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "23\n",
      "28\n",
      "29\n",
      "31\n",
      "33\n",
      "37\n",
      "40\n",
      "44\n",
      "47\n",
      "54\n",
      "61\n",
      "74\n",
      "80\n",
      "83\n",
      "90\n",
      "98\n",
      "105\n",
      "115\n",
      "126\n",
      "148\n",
      "165\n",
      "183\n",
      "201\n",
      "226\n",
      "251\n",
      "278\n",
      "314\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-23'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-23'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-23'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-23'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-231685'\n",
    "REPLAY_RE = 'replay-1-231685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred23'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred23'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_settings = {\n",
    "    #earlier objects are eaten by later objects (pred eat prey)\n",
    "    'objects': [\n",
    "        'prey',\n",
    "        'pred',\n",
    "        'cue',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'prey': [212, 211, 208],\n",
    "        'pred':  [100, 37, 0],\n",
    "        'cue': [0,0,0],\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'prey': {'prey': 0.1, 'pred': -0.1, 'cue': 0.0},\n",
    "        'pred': {'prey': 1.0, 'pred': -1.0, 'cue': 0.0},\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (500,300),   \n",
    "    \"maximum_velocity\":      {'prey': 0, 'pred': 50},\n",
    "    \"object_radius\": 10.0,\n",
    "    \"cue_types\": 2,\n",
    "    \"num_objects\": OrderedDict([('prey', 5), ('pred', 1), ('cue', 1)]),\n",
    "    # active means that the objects are learning\n",
    "    \"num_objects_active\": OrderedDict([('prey', 0), ('pred', 1)]), \n",
    "    #'multiple' to create each DQN for each prey/predator\n",
    "    #'one' to use one DQN for all preys/predators\n",
    "    # only really matters if the preys/predators are active\n",
    "    \"network_prey\": 'one',\n",
    "    \"network_pred\": 'multiple',\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 75.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -1.0,\n",
    "    \"delta_v\": 50\n",
    "}\n",
    "\n",
    "#'new' to create new sim with values above\n",
    "#'load' to load a previously trained graph\n",
    "RUN = 'new'  \n",
    "\n",
    "# First three for names for saving new runs\n",
    "MODEL_NAME = 'model-1pred-2cues-500trials-24'\n",
    "REPLAY_NAME = 'replay-1pred-2cues-500trials-24'\n",
    "ELAPSE_NAME = 'elapse-1pred-2cues-500trials-24'\n",
    "REWARDS_NAME = 'rewards-1pred-2cues-500trials-24'\n",
    "\n",
    "# Last two for names for reloading model/replay buffers\n",
    "MODEL_RE = 'model-1-241685'\n",
    "REPLAY_RE = 'replay-1-241685'\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "all_act = []\n",
    "all_train = []\n",
    "all_update = []\n",
    "all_debug = []\n",
    "all_replay = []\n",
    "\n",
    "# Build graphs\n",
    "if current_settings['num_objects_active']['prey'] != 0:\n",
    "    if current_settings['network_prey'] == 'one':\n",
    "        network_prey = 1\n",
    "    else:\n",
    "        network_prey = current_settings['num_objects_active']['prey']\n",
    "\n",
    "    for i in range(network_prey):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            brain_prey = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_prey,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "if current_settings['num_objects_active']['pred'] != 0:\n",
    "    if current_settings['network_pred'] == 'one':\n",
    "        network_pred = 1\n",
    "    else:\n",
    "        network_pred = current_settings['num_objects_active']['pred']\n",
    "\n",
    "    for i in range(network_pred):\n",
    "        # CHANGE THIS\n",
    "        name = 'pred24'\n",
    "        with tf.variable_scope(name):\n",
    "            brain_pred = deepq.models.mlp([200, 200])\n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput((g.observation_size,), name=name),\n",
    "                q_func=brain_pred,\n",
    "                num_actions=g.num_actions,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "            )\n",
    "        replay_buffer = ReplayBuffer(50000)\n",
    "        all_replay.append(replay_buffer)\n",
    "        all_act.append(act)\n",
    "        all_train.append(train)\n",
    "        all_update.append(update_target)\n",
    "        all_debug.append(debug)\n",
    "\n",
    "# Simulate\n",
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 100\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "elapsed = []\n",
    "rewards = []\n",
    "timesteps = [0]\n",
    "    \n",
    "# Initializing or reloading variables\n",
    "# Start TensorFlow session with 2 CPUs\n",
    "with U.make_session(2) as sess:\n",
    "    \n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    for i in range(current_settings['num_objects_active']['prey']):\n",
    "        name = 'prey' + str(i)\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "    for i in range(current_settings['num_objects_active']['pred']):\n",
    "        name = 'pred24'\n",
    "        with tf.variable_scope(name):\n",
    "            update_target()\n",
    "            \n",
    "    if RUN == 'load':\n",
    "#         # when only restoring a subset of variables\n",
    "#         restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'pred0')\n",
    "#         saver = tf.train.Saver(restore)\n",
    "\n",
    "        # reload models\n",
    "        saver = tf.train.Saver()\n",
    "        current_dir = os.getcwd()\n",
    "        model_name = current_dir + '/saved_graphs/' + MODEL_RE + '.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        # reload replay buffers\n",
    "        replay_name = current_dir + '/saved_graphs/' + REPLAY_RE + '.pkl'\n",
    "        with open(replay_name, 'rb') as f:\n",
    "            all_replay = pickle.load(f)\n",
    "            \n",
    "#         # remember to append buffer if restoring a subset of variables\n",
    "#         all_replay.append(replay_buffer)\n",
    "    \n",
    "    # Run simulation\n",
    "    try:\n",
    "        simulate(simulation=g,\n",
    "                 replay = all_replay,\n",
    "                 act = all_act,\n",
    "                 train = all_train,\n",
    "                 update = all_update,\n",
    "                 debug = all_debug,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=.001,\n",
    "                 save_path=None,\n",
    "                 timesteps = timesteps,\n",
    "                 elapsed = elapsed,\n",
    "                 all_rewards = rewards)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        g.shut_down_graphics()\n",
    "        print('graphics shut down')\n",
    "        \n",
    "    # Save models    \n",
    "    model_name = 'saved_graphs/' + MODEL_NAME + '-' + str(timesteps[0]) + '.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, model_name)\n",
    "    \n",
    "    # Save replay buffers\n",
    "    replay_name = 'saved_graphs/' + REPLAY_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(replay_name, \"wb\") as f:\n",
    "        pickle.dump(all_replay, f)\n",
    "        \n",
    "    # Save trial times\n",
    "    elapse_name = 'saved_graphs/' + ELAPSE_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(elapse_name, \"wb\") as f:\n",
    "        pickle.dump(elapsed, f)\n",
    "        \n",
    "    # Save rewards\n",
    "    rewards_name = 'saved_graphs/' + REWARDS_NAME + '-' + str(timesteps[0]) + '.pkl'\n",
    "    with open(rewards_name, \"wb\") as f:\n",
    "        pickle.dump(rewards, f)\n",
    "\n",
    "%reset -f \n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, ModelController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from collections import OrderedDict\n",
    "from euclid import Vector2\n",
    "\n",
    "from baselines import deepq\n",
    "import baselines.common.tf_util as U\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (dqn-multiagent)",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
